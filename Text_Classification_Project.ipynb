{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c499e94e",
   "metadata": {},
   "source": [
    "# Text Classification with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040774dd",
   "metadata": {},
   "source": [
    "### Importing, preparing and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df15653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jannik\\Coding Ninjas\\Machine Learning\\Naive Bayes\n",
      "['.ipynb_checkpoints', '20_newsgroups.tar', 'Naive_Bayes.ipynb', 'Notes.ipynb', 'Text_Classification_Project.ipynb']\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# finding path of data\n",
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir('.'))\n",
    "print(os.listdir('C:/Users/Jannik/Coding Ninjas/Machine Learning/Naive Bayes/20_newsgroups.tar/20_newsgroups'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "655a0abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating access to all data in all files with dictionary\n",
    "lst_names = []\n",
    "d = {}\n",
    "\n",
    "for (root,dirs,files) in os.walk('C:/Users/Jannik/Coding Ninjas/Machine Learning/Naive Bayes/20_newsgroups.tar/20_newsgroups'):\n",
    "     for name in dirs:\n",
    "        lst_names.append(name)\n",
    "        \n",
    "for key in lst_names:\n",
    "    d[key] = []\n",
    "    \n",
    "for key in d.keys():\n",
    "    for (root,dirs,files) in os.walk('C:/Users/Jannik/Coding Ninjas/Machine Learning/Naive Bayes/20_newsgroups.tar/20_newsgroups/{}'.format(key)):\n",
    "         for name in files:\n",
    "            d[key].append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26b1e576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5efe066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing stopwords from nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0763c209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creating vocabulary for 80% of documents of every class\n",
    "direction = 'C:/Users/Jannik/Coding Ninjas/Machine Learning/Naive Bayes/20_newsgroups.tar/20_newsgroups'\n",
    "import re\n",
    "def create_voc(dic,direction):\n",
    "    voc = {} \n",
    "    for i in range(0,len(dic)):\n",
    "        train_split = int(0.8 * len(list(dic.values())[i]))\n",
    "        # just to see work progress\n",
    "        print(str(i+1)+'/'+ str(len(dic)),end=' ')\n",
    "        for j in range(0,train_split):\n",
    "            data = open('{}/{}/{}'.format(direction,(list(dic.keys())[i]),(list(dic.values())[i][j])))\n",
    "            data = data.read()\n",
    "            data = data.lower()\n",
    "            # extract words from string using regex\n",
    "            split = re.findall(r'\\w+', data)\n",
    "            for word in split:\n",
    "                if word not in stopwords:\n",
    "                    if word not in voc:\n",
    "                        voc[word] = 1\n",
    "                    else:\n",
    "                        voc[word] += 1\n",
    "    # sorting voc\n",
    "    sort_voc = sorted(voc.items(), key=lambda x: x[1], reverse=True)\n",
    "    # just considering 1000 most important words\n",
    "    new_voc = {}\n",
    "    for i in range(1000):\n",
    "        new_voc[sort_voc[i][0]] = sort_voc[i][1]\n",
    "    \n",
    "    # deleting unnecesary space use\n",
    "    del(voc)\n",
    "    del(sort_voc)\n",
    "        \n",
    "    return new_voc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f24487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the vocabulary from our training data\n",
    "#vocabulary = create_voc(d,direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06c2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking vocabulary\n",
    "#print(vocabulary)\n",
    "#len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee5ef2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 2d arrays for data using vocabulary and our dictionary/direction to access all the data\n",
    "def create_array(voc,dic,direction,test=False):\n",
    "    # creating x,y and feature arrays\n",
    "    x = []\n",
    "    y = []\n",
    "    features = [key for key in voc.keys()]\n",
    "    \n",
    "    # set row count index independent from test or train split\n",
    "    c = 0\n",
    "    \n",
    "    # if test is True, we are creating test arrays\n",
    "    for i in range(0,len(dic)):\n",
    "        if test == False:\n",
    "            array_split = int(0.8 * len(list(dic.values())[i]))\n",
    "            start = 0\n",
    "        else:\n",
    "            array_split = len(list(dic.values())[i])\n",
    "            start = array_split - (array_split - (int(0.8 * array_split)))\n",
    "            \n",
    "        # just to see work progress\n",
    "        print(str(i+1)+'/'+ str(len(dic)),end=' ')\n",
    "        \n",
    "        for j in range(start,array_split):\n",
    "            y.append(list(dic.keys())[i])\n",
    "            data = open('{}/{}/{}'.format(direction,(list(dic.keys())[i]),(list(dic.values())[i][j])))\n",
    "            data = data.read()\n",
    "            data = data.lower()\n",
    "            # extract words from documents of each class using regex and put counts in x_array\n",
    "            split = re.findall(r'\\w+', data)\n",
    "            x.append([0 for f in range(len(features))])\n",
    "            for g in range(len(split)):\n",
    "                if split[g] in features:\n",
    "                    index = features.index(split[g])\n",
    "                    x[c][index] += 1\n",
    "            c += 1\n",
    "    return x,y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c13eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f0db07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating training arrays and converting them to numpy arrays\n",
    "#x_train,y_train = create_array(vocabulary,d,direction,test=False)\n",
    "#x_train = np.array(x_train)\n",
    "#y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd529542",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = 13039\n",
    "#x_train[n,:], y_train[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2b8667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating testing arrays and converting them to numpy arrays\n",
    "#x_test,y_test = create_array(vocabulary,d,direction,test=True)\n",
    "#x_test = np.array(x_test)\n",
    "#y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5693e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "075ea851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking validity of arrays\n",
    "#len(x_test),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41a448e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking validity of arrays\n",
    "#len(x_train),len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a8671",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c5474d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit function for classifier with training data\n",
    "def fit(x_train,y_train):\n",
    "    result = {}\n",
    "    count_classes = set(y_train)\n",
    "    for current_class in count_classes:\n",
    "        result[current_class] = {}\n",
    "        result['total_data'] = len(y_train)\n",
    "        num_features = x_train.shape[1]\n",
    "        curr_class_rows =  (y_train == current_class)\n",
    "        x_train_current = x_train[curr_class_rows]\n",
    "        y_train_current = y_train[curr_class_rows]\n",
    "        result[current_class]['total_count_class'] = len(y_train_current)\n",
    "        for j in range(1, num_features +1):\n",
    "            result[current_class][j] = {}\n",
    "            all_possible_values = set(x_train[:,j-1])\n",
    "            for current_value in all_possible_values:\n",
    "                result[current_class][j][current_value] = (current_value == x_train_current[:,j-1]).sum()\n",
    "                \n",
    "    return result   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3d6e127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding probabilities of current class\n",
    "def probabilities(x,dictionary,current_class):\n",
    "    # total data belonging to current class devided with total data (2nd term of formula)\n",
    "    output = np.log(dictionary[current_class]['total_count_class']) - np.log(dictionary['total_data'])\n",
    "    num_features = len(dictionary[current_class].keys())-1\n",
    "    for j in range(1,num_features + 1):\n",
    "        xj = x[j-1]\n",
    "        #print('xj',xj)\n",
    "        # error exception\n",
    "        if xj not in dictionary[current_class][j].keys():\n",
    "            count_current_class_with_value_xj = 1\n",
    "        else:    \n",
    "            # with lapace correction added\n",
    "            count_current_class_with_value_xj = dictionary[current_class][j][xj]  + 1 \n",
    "        count_current_class = dictionary[current_class]['total_count_class'] + len(dictionary[current_class][j].keys())\n",
    "        # 1st term of formula\n",
    "        prob_xj = np.log(count_current_class_with_value_xj) - np.log(count_current_class)\n",
    "        output += prob_xj\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce7df327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find prediction for single x points\n",
    "def predict_single_point(dictionary,x):\n",
    "    # initial values\n",
    "    best_p = -1000\n",
    "    best_class = -1\n",
    "    classes = dictionary.keys()\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        #print('current_class:', current_class)\n",
    "        # avoiding total data as a class\n",
    "        if current_class == 'total_data':\n",
    "            continue\n",
    "        p_current_class = probabilities(x,dictionary,current_class)\n",
    "        if (first_run or p_current_class > best_p):\n",
    "            best_p = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7787e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict function\n",
    "def predict(dictionary,x_test):\n",
    "    y_pred = []\n",
    "    # progress count \n",
    "    c = 0\n",
    "    print('predict progress: ')\n",
    "    for x in x_test:\n",
    "        c += 1\n",
    "        # progress overview\n",
    "        if c % 100 == 0:\n",
    "            print(c, '/', len(x_test), end=' ')\n",
    "        x_class = predict_single_point(dictionary,x)\n",
    "        y_pred.append(x_class)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482f41e",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62c97fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary progress:\n",
      "1/20 2/20 3/20 4/20 5/20 6/20 7/20 8/20 9/20 10/20 11/20 12/20 13/20 14/20 15/20 16/20 17/20 18/20 19/20 20/20 \n",
      "training arrays progress:\n",
      "1/20 2/20 3/20 4/20 5/20 6/20 7/20 8/20 9/20 10/20 11/20 12/20 13/20 14/20 15/20 16/20 17/20 18/20 19/20 20/20 \n",
      "testing array progress:\n",
      "1/20 2/20 3/20 4/20 5/20 6/20 7/20 8/20 9/20 10/20 11/20 12/20 13/20 14/20 15/20 16/20 17/20 18/20 19/20 20/20 "
     ]
    }
   ],
   "source": [
    "#creating the vocabulary from our training data from the computer\n",
    "print('vocabulary progress:')\n",
    "vocabulary = create_voc(d,direction)\n",
    "print()\n",
    "\n",
    "#creating training arrays and converting them to numpy arrays\n",
    "print('training arrays progress:')\n",
    "x_train,y_train = create_array(vocabulary,d,direction,test=False)\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "print()\n",
    "\n",
    "# fit function for classifier with training data\n",
    "dictionary = fit(x_train,y_train)\n",
    "\n",
    "# creating testing arrays and converting them to numpy arrays\n",
    "print('testing array progress:')\n",
    "x_test,y_test = create_array(vocabulary,d,direction,test=True)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a7cef33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict progress: \n",
      "100 / 4000 200 / 4000 300 / 4000 400 / 4000 500 / 4000 600 / 4000 700 / 4000 800 / 4000 900 / 4000 1000 / 4000 1100 / 4000 1200 / 4000 1300 / 4000 1400 / 4000 1500 / 4000 1600 / 4000 1700 / 4000 1800 / 4000 1900 / 4000 2000 / 4000 2100 / 4000 2200 / 4000 2300 / 4000 2400 / 4000 2500 / 4000 2600 / 4000 2700 / 4000 2800 / 4000 2900 / 4000 3000 / 4000 3100 / 4000 3200 / 4000 3300 / 4000 3400 / 4000 3500 / 4000 3600 / 4000 3700 / 4000 3800 / 4000 3900 / 4000 4000 / 4000 "
     ]
    }
   ],
   "source": [
    "# prediction y_pred for x_test data\n",
    "y_pred = predict(dictionary,x_test)\n",
    "#print(set(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19669093",
   "metadata": {},
   "source": [
    "### sklearn build in function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d374324",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gaussian Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf_Gau = GaussianNB()\n",
    "clf_Gau.fit(x_train,y_train)\n",
    "y_pred_Gau = clf_Gau.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80de6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinominal Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_multi = MultinomialNB()\n",
    "clf_multi.fit(x_train,y_train)\n",
    "y_pred_multi = clf_multi.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a05f6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b9837",
   "metadata": {},
   "source": [
    "## comparing both algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c71c9374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.64      0.82      0.72       200\n",
      "           comp.graphics       0.42      0.25      0.31       200\n",
      " comp.os.ms-windows.misc       0.55      0.27      0.36       200\n",
      "comp.sys.ibm.pc.hardware       0.26      0.37      0.31       200\n",
      "   comp.sys.mac.hardware       0.29      0.55      0.38       200\n",
      "          comp.windows.x       0.58      0.19      0.29       200\n",
      "            misc.forsale       0.42      0.78      0.55       200\n",
      "               rec.autos       0.72      0.56      0.63       200\n",
      "         rec.motorcycles       0.68      0.90      0.78       200\n",
      "      rec.sport.baseball       0.69      0.82      0.75       200\n",
      "        rec.sport.hockey       0.86      0.76      0.80       200\n",
      "               sci.crypt       0.90      0.95      0.92       200\n",
      "         sci.electronics       0.47      0.53      0.50       200\n",
      "                 sci.med       0.79      0.38      0.51       200\n",
      "               sci.space       0.82      0.58      0.68       200\n",
      "  soc.religion.christian       0.98      0.95      0.97       200\n",
      "      talk.politics.guns       0.56      0.54      0.55       200\n",
      "   talk.politics.mideast       0.76      0.84      0.80       200\n",
      "      talk.politics.misc       0.58      0.32      0.41       200\n",
      "      talk.religion.misc       0.44      0.46      0.45       200\n",
      "\n",
      "                accuracy                           0.59      4000\n",
      "               macro avg       0.62      0.59      0.58      4000\n",
      "            weighted avg       0.62      0.59      0.58      4000\n",
      "\n",
      "[[164   0   0   0   0   0   0   0   0   0   0   0   0   1   0   2   0   3\n",
      "    4  26]\n",
      " [  0  50   9  39  38   5  31   0   0   2   0   8  12   3   2   0   1   0\n",
      "    0   0]\n",
      " [  0   5  54  66  48   7  18   1   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   6  11  74  62   4  26   0   0   0   0   0  17   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   3   3  31 109   1  34   0   1   0   0   3  13   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0  24  13  52  50  38  14   0   2   0   0   0   5   1   1   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1  12  11   1 155   6   1   0   0   0  12   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0  15 113  33   0   3   0  29   0   0   0   7   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   8  11 180   0   0   0   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   7   0   2 165  20   0   0   1   1   0   4   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   5   0   0  43 151   0   0   0   0   0   0   1\n",
      "    0   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0 190   0   0   0   0   8   0\n",
      "    0   0]\n",
      " [  0   7   3   7  15   9  21  16   0   0   0   1 106   1   4   0   0   7\n",
      "    0   3]\n",
      " [  0  10   4   2  32   1  20   0  11  19   0   2  15  76   3   0   0   0\n",
      "    1   4]\n",
      " [  5  13   0   2   5   0  12   7   2   7   0   2  17   5 117   0   1   2\n",
      "    1   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 191   0   0\n",
      "    2   7]\n",
      " [  0   0   0   0   0   0   0   2  26   0   0   3   0   0   8   0 108   6\n",
      "   13  34]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   6 169\n",
      "   21   4]\n",
      " [  2   0   0   0   0   0   0   1   5   2   1   2   0   7   5   0  49  28\n",
      "   64  34]\n",
      " [ 85   1   0   0   0   0   2   0   0   0   1   0   0   0   1   1   9   5\n",
      "    4  91]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn in build  Gaussian classifier \n",
    "print(classification_report(y_test,y_pred_Gau))\n",
    "print(confusion_matrix(y_test,y_pred_Gau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b62435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.78      0.62      0.69       200\n",
      "           comp.graphics       0.65      0.76      0.70       200\n",
      " comp.os.ms-windows.misc       0.85      0.52      0.64       200\n",
      "comp.sys.ibm.pc.hardware       0.63      0.79      0.70       200\n",
      "   comp.sys.mac.hardware       0.78      0.90      0.83       200\n",
      "          comp.windows.x       0.86      0.62      0.72       200\n",
      "            misc.forsale       0.78      0.93      0.85       200\n",
      "               rec.autos       0.88      0.84      0.86       200\n",
      "         rec.motorcycles       0.77      0.93      0.84       200\n",
      "      rec.sport.baseball       0.82      0.86      0.84       200\n",
      "        rec.sport.hockey       0.92      0.71      0.81       200\n",
      "               sci.crypt       0.95      0.81      0.88       200\n",
      "         sci.electronics       0.67      0.80      0.73       200\n",
      "                 sci.med       0.91      0.83      0.87       200\n",
      "               sci.space       0.88      0.84      0.86       200\n",
      "  soc.religion.christian       0.94      1.00      0.97       200\n",
      "      talk.politics.guns       0.76      0.91      0.83       200\n",
      "   talk.politics.mideast       0.86      0.92      0.88       200\n",
      "      talk.politics.misc       0.81      0.59      0.68       200\n",
      "      talk.religion.misc       0.63      0.71      0.67       200\n",
      "\n",
      "                accuracy                           0.79      4000\n",
      "               macro avg       0.81      0.79      0.79      4000\n",
      "            weighted avg       0.81      0.79      0.79      4000\n",
      "\n",
      "[[124   0   0   1   2   0   0   3   4   0   0   0   4   2   3   1   0   3\n",
      "    0  53]\n",
      " [  1 151   3   7  13   6   4   0   0   0   0   4   7   2   2   0   0   0\n",
      "    0   0]\n",
      " [  0  21 104  49   7  12   5   0   0   0   0   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   7   4 157  18   0   5   0   0   0   0   0   7   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   2   1  11 180   0   5   0   0   0   0   0   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0  34   6  17   5 124   2   1   3   0   0   2   2   1   3   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   3   0   0 185   1   2   0   3   0   5   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   0   5 169   7   0   2   0  13   0   2   0   1   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   9   2 186   0   0   0   3   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   4   4   6 172   7   0   2   0   0   0   4   0\n",
      "    1   0]\n",
      " [  0   0   1   0   0   0   3   1  16  34 143   0   1   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   4   0   2   0   2   3   1   3   1   0 163  14   1   2   0   2   0\n",
      "    2   0]\n",
      " [  0   5   2   2   1   0   1   3   3   1   0   0 160   2   3   0   0  17\n",
      "    0   0]\n",
      " [  0   4   1   0   4   0   1   4   5   3   0   1  11 166   0   0   0   0\n",
      "    0   0]\n",
      " [  3   2   0   0   2   0   0   3   2   0   0   0   5   7 169   0   0   1\n",
      "    4   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 200   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   1   0   3   0   0   0   0   0   0   0 182   0\n",
      "    4   9]\n",
      " [  1   0   1   0   0   0   0   1   0   0   0   0   1   0   2   0   3 183\n",
      "    8   0]\n",
      " [  0   1   0   0   0   0   1   0   2   0   0   0   1   1   2   2  41  10\n",
      "  118  21]\n",
      " [ 29   1   0   0   0   0   3   0   0   0   0   0   0   0   1   9   5   0\n",
      "    9 143]]\n"
     ]
    }
   ],
   "source": [
    "# mulitnomial Classifier\n",
    "print(classification_report(y_test,y_pred_multi))\n",
    "print(confusion_matrix(y_test,y_pred_multi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcc6a9a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.87      0.69      0.77       200\n",
      "           comp.graphics       0.95      0.89      0.92       200\n",
      " comp.os.ms-windows.misc       0.93      0.98      0.96       200\n",
      "comp.sys.ibm.pc.hardware       0.99      0.99      0.99       200\n",
      "   comp.sys.mac.hardware       0.99      0.97      0.98       200\n",
      "          comp.windows.x       0.94      0.93      0.93       200\n",
      "            misc.forsale       0.82      0.97      0.89       200\n",
      "               rec.autos       0.99      0.98      0.99       200\n",
      "         rec.motorcycles       0.98      0.98      0.98       200\n",
      "      rec.sport.baseball       1.00      0.96      0.98       200\n",
      "        rec.sport.hockey       0.98      0.99      0.99       200\n",
      "               sci.crypt       0.96      0.83      0.89       200\n",
      "         sci.electronics       0.87      0.96      0.91       200\n",
      "                 sci.med       0.97      0.97      0.97       200\n",
      "               sci.space       0.96      0.94      0.95       200\n",
      "  soc.religion.christian       1.00      1.00      1.00       200\n",
      "      talk.politics.guns       0.87      0.91      0.89       200\n",
      "   talk.politics.mideast       0.92      0.99      0.95       200\n",
      "      talk.politics.misc       0.81      0.62      0.70       200\n",
      "      talk.religion.misc       0.68      0.83      0.75       200\n",
      "\n",
      "                accuracy                           0.92      4000\n",
      "               macro avg       0.92      0.92      0.92      4000\n",
      "            weighted avg       0.92      0.92      0.92      4000\n",
      "\n",
      "[[139   1   0   0   0   1   1   0   2   0   0   0   1   1   0   0   0   1\n",
      "    2  51]\n",
      " [  4 178   3   0   1   7   1   0   0   0   0   3   0   1   1   0   0   0\n",
      "    1   0]\n",
      " [  0   0 197   0   0   2   0   0   0   0   0   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0 199   0   0   1   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   1 194   0   2   0   0   0   0   1   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   4  11   0   0 185   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   0   1   0   0 194   0   0   0   0   1   2   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   2 197   1   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   3   0 196   0   0   0   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   1   0   1 193   4   0   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0 199   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   1   0   0   1   3   0   0   0   0 167  22   2   0   0   1   0\n",
      "    2   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1 193   1   4   0   0   0\n",
      "    1   0]\n",
      " [  2   1   0   0   0   0   2   0   0   0   0   0   0 194   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   4   0   0   0   0   0   4   0 188   0   0   0\n",
      "    4   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 200   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   3   0   0   0   0   0   0   0   0   0 181   0\n",
      "   10   6]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 198\n",
      "    1   0]\n",
      " [  0   0   0   0   0   0  12   1   1   0   0   0   0   0   0   0  24  15\n",
      "  124  23]\n",
      " [ 14   0   0   0   0   1   9   0   0   0   0   0   0   0   1   0   3   1\n",
      "    4 167]]\n"
     ]
    }
   ],
   "source": [
    "# self implementated classifier\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419db06",
   "metadata": {},
   "source": [
    "### self implemented classifier has the highest accuracy but is very slowly, followed by the  fast build-in multinomial classifier. The Gaussian Classifier is not very accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
